{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h2o in c:\\users\\user\\anaconda3\\lib\\site-packages (3.42.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from h2o) (2.28.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\user\\anaconda3\\lib\\site-packages (from h2o) (0.8.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->h2o) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->h2o) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->h2o) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->h2o) (2.0.4)\n",
      "Requirement already satisfied: pandasql in c:\\users\\user\\anaconda3\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandasql) (1.23.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandasql) (1.5.3)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandasql) (1.4.39)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->pandasql) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->pandasql) (2022.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sqlalchemy->pandasql) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->pandasql) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install h2o\n",
    "!pip install pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================\n",
    "# 패키지 로딩\n",
    "#============================================================\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import h2o\n",
    "import numpy as np\n",
    "import itertools \n",
    "import math\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "from pandasql import sqldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir : C:\\Users\\user\\content\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# 분석 환경 셋팅\n",
    "#============================================================\n",
    "sys.stdout.flush() #Python 메모리에 생성된 모든 객체 삭제(초기화)\n",
    "\n",
    "#============================================================\n",
    "# 작업 디렉토리 경로 확인\n",
    "#============================================================\n",
    "currentPath=os.getcwd()\n",
    "print('Current working dir : %s' % currentPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YYMMDD</th>\n",
       "      <th>HAEGU_NUM</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>AVG_EMP</th>\n",
       "      <th>MAX_SSH</th>\n",
       "      <th>AVG_SURFACE_SALINITY_TREND</th>\n",
       "      <th>STDEV_SURFACE_TEMPERATURE_TREND</th>\n",
       "      <th>AVG_SALINITY_01</th>\n",
       "      <th>AVG_SALINITY_02</th>\n",
       "      <th>...</th>\n",
       "      <th>MAX_V_VELOCITY_03</th>\n",
       "      <th>MAX_V_VELOCITY_04</th>\n",
       "      <th>MIN_V_VELOCITY_01</th>\n",
       "      <th>MIN_V_VELOCITY_02</th>\n",
       "      <th>MIN_V_VELOCITY_03</th>\n",
       "      <th>MIN_V_VELOCITY_04</th>\n",
       "      <th>STDEV_V_VELOCITY_01</th>\n",
       "      <th>STDEV_V_VELOCITY_02</th>\n",
       "      <th>STDEV_V_VELOCITY_03</th>\n",
       "      <th>STDEV_V_VELOCITY_04</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-09-20</td>\n",
       "      <td>97</td>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>0.259959</td>\n",
       "      <td>0.019379</td>\n",
       "      <td>0.131165</td>\n",
       "      <td>31.6075</td>\n",
       "      <td>31.6081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017062</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-0.121754</td>\n",
       "      <td>-0.035654</td>\n",
       "      <td>-0.017062</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.032370</td>\n",
       "      <td>0.014972</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-09-20</td>\n",
       "      <td>98</td>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.240687</td>\n",
       "      <td>0.061153</td>\n",
       "      <td>0.880088</td>\n",
       "      <td>32.8018</td>\n",
       "      <td>32.8243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072945</td>\n",
       "      <td>0.031431</td>\n",
       "      <td>-0.036622</td>\n",
       "      <td>-0.033818</td>\n",
       "      <td>-0.038480</td>\n",
       "      <td>-0.041347</td>\n",
       "      <td>0.034178</td>\n",
       "      <td>0.022088</td>\n",
       "      <td>0.040056</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-09-20</td>\n",
       "      <td>99</td>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.258393</td>\n",
       "      <td>-0.242847</td>\n",
       "      <td>0.178685</td>\n",
       "      <td>32.8512</td>\n",
       "      <td>32.8829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407543</td>\n",
       "      <td>0.444247</td>\n",
       "      <td>-0.272650</td>\n",
       "      <td>-0.173565</td>\n",
       "      <td>-0.014867</td>\n",
       "      <td>-0.079175</td>\n",
       "      <td>0.244142</td>\n",
       "      <td>0.187899</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.143455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-09-20</td>\n",
       "      <td>213</td>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.249247</td>\n",
       "      <td>-0.293269</td>\n",
       "      <td>1.727980</td>\n",
       "      <td>32.4543</td>\n",
       "      <td>32.5524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131133</td>\n",
       "      <td>0.130061</td>\n",
       "      <td>-0.122278</td>\n",
       "      <td>-0.072258</td>\n",
       "      <td>-0.061687</td>\n",
       "      <td>-0.031458</td>\n",
       "      <td>0.066656</td>\n",
       "      <td>0.040829</td>\n",
       "      <td>0.068074</td>\n",
       "      <td>0.050827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-09-20</td>\n",
       "      <td>214</td>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.239578</td>\n",
       "      <td>-0.443171</td>\n",
       "      <td>1.554340</td>\n",
       "      <td>32.7638</td>\n",
       "      <td>32.8194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062962</td>\n",
       "      <td>0.065815</td>\n",
       "      <td>-0.143893</td>\n",
       "      <td>-0.088814</td>\n",
       "      <td>-0.074735</td>\n",
       "      <td>-0.009163</td>\n",
       "      <td>0.057059</td>\n",
       "      <td>0.039952</td>\n",
       "      <td>0.046266</td>\n",
       "      <td>0.029428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12750</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>97</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0.141733</td>\n",
       "      <td>0.017642</td>\n",
       "      <td>0.016048</td>\n",
       "      <td>33.9187</td>\n",
       "      <td>33.9196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-0.042890</td>\n",
       "      <td>-0.005009</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.012369</td>\n",
       "      <td>0.008401</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12751</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>98</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>0.151005</td>\n",
       "      <td>1.913620</td>\n",
       "      <td>1.995750</td>\n",
       "      <td>33.9292</td>\n",
       "      <td>33.9454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029539</td>\n",
       "      <td>0.023992</td>\n",
       "      <td>-0.060395</td>\n",
       "      <td>-0.038716</td>\n",
       "      <td>-0.015458</td>\n",
       "      <td>0.005838</td>\n",
       "      <td>0.029858</td>\n",
       "      <td>0.026927</td>\n",
       "      <td>0.016806</td>\n",
       "      <td>0.007840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12752</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>99</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>0.203799</td>\n",
       "      <td>0.427979</td>\n",
       "      <td>2.624260</td>\n",
       "      <td>33.8959</td>\n",
       "      <td>33.9022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422083</td>\n",
       "      <td>0.378267</td>\n",
       "      <td>-0.075958</td>\n",
       "      <td>-0.020391</td>\n",
       "      <td>-0.007830</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>0.119245</td>\n",
       "      <td>0.115518</td>\n",
       "      <td>0.108240</td>\n",
       "      <td>0.098283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12753</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>213</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>0.186330</td>\n",
       "      <td>0.040789</td>\n",
       "      <td>0.050849</td>\n",
       "      <td>33.0969</td>\n",
       "      <td>33.0963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076891</td>\n",
       "      <td>0.060830</td>\n",
       "      <td>-0.091773</td>\n",
       "      <td>-0.044862</td>\n",
       "      <td>-0.021534</td>\n",
       "      <td>-0.015512</td>\n",
       "      <td>0.039020</td>\n",
       "      <td>0.034755</td>\n",
       "      <td>0.028141</td>\n",
       "      <td>0.022072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12754</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>214</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>0.192848</td>\n",
       "      <td>0.054854</td>\n",
       "      <td>0.287741</td>\n",
       "      <td>33.1854</td>\n",
       "      <td>33.1831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061532</td>\n",
       "      <td>0.060296</td>\n",
       "      <td>-0.122055</td>\n",
       "      <td>-0.079896</td>\n",
       "      <td>-0.060736</td>\n",
       "      <td>-0.047039</td>\n",
       "      <td>0.049107</td>\n",
       "      <td>0.044644</td>\n",
       "      <td>0.036002</td>\n",
       "      <td>0.035390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12755 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           YYMMDD  HAEGU_NUM  year  month   AVG_EMP   MAX_SSH  \\\n",
       "0      2008-09-20         97  2008      9 -0.000068  0.259959   \n",
       "1      2008-09-20         98  2008      9  0.000088  0.240687   \n",
       "2      2008-09-20         99  2008      9  0.000333  0.258393   \n",
       "3      2008-09-20        213  2008      9  0.000091  0.249247   \n",
       "4      2008-09-20        214  2008      9  0.000153  0.239578   \n",
       "...           ...        ...   ...    ...       ...       ...   \n",
       "12750  2016-12-31         97  2016     12 -0.000072  0.141733   \n",
       "12751  2016-12-31         98  2016     12 -0.000191  0.151005   \n",
       "12752  2016-12-31         99  2016     12 -0.000115  0.203799   \n",
       "12753  2016-12-31        213  2016     12 -0.000265  0.186330   \n",
       "12754  2016-12-31        214  2016     12 -0.000398  0.192848   \n",
       "\n",
       "       AVG_SURFACE_SALINITY_TREND  STDEV_SURFACE_TEMPERATURE_TREND  \\\n",
       "0                        0.019379                         0.131165   \n",
       "1                        0.061153                         0.880088   \n",
       "2                       -0.242847                         0.178685   \n",
       "3                       -0.293269                         1.727980   \n",
       "4                       -0.443171                         1.554340   \n",
       "...                           ...                              ...   \n",
       "12750                    0.017642                         0.016048   \n",
       "12751                    1.913620                         1.995750   \n",
       "12752                    0.427979                         2.624260   \n",
       "12753                    0.040789                         0.050849   \n",
       "12754                    0.054854                         0.287741   \n",
       "\n",
       "       AVG_SALINITY_01  AVG_SALINITY_02  ...  MAX_V_VELOCITY_03  \\\n",
       "0              31.6075          31.6081  ...          -0.017062   \n",
       "1              32.8018          32.8243  ...           0.072945   \n",
       "2              32.8512          32.8829  ...           0.407543   \n",
       "3              32.4543          32.5524  ...           0.131133   \n",
       "4              32.7638          32.8194  ...           0.062962   \n",
       "...                ...              ...  ...                ...   \n",
       "12750          33.9187          33.9196  ...           0.000251   \n",
       "12751          33.9292          33.9454  ...           0.029539   \n",
       "12752          33.8959          33.9022  ...           0.422083   \n",
       "12753          33.0969          33.0963  ...           0.076891   \n",
       "12754          33.1854          33.1831  ...           0.061532   \n",
       "\n",
       "       MAX_V_VELOCITY_04  MIN_V_VELOCITY_01  MIN_V_VELOCITY_02  \\\n",
       "0            -999.000000          -0.121754          -0.035654   \n",
       "1               0.031431          -0.036622          -0.033818   \n",
       "2               0.444247          -0.272650          -0.173565   \n",
       "3               0.130061          -0.122278          -0.072258   \n",
       "4               0.065815          -0.143893          -0.088814   \n",
       "...                  ...                ...                ...   \n",
       "12750        -999.000000          -0.042890          -0.005009   \n",
       "12751           0.023992          -0.060395          -0.038716   \n",
       "12752           0.378267          -0.075958          -0.020391   \n",
       "12753           0.060830          -0.091773          -0.044862   \n",
       "12754           0.060296          -0.122055          -0.079896   \n",
       "\n",
       "       MIN_V_VELOCITY_03  MIN_V_VELOCITY_04  STDEV_V_VELOCITY_01  \\\n",
       "0              -0.017062        -999.000000             0.032370   \n",
       "1              -0.038480          -0.041347             0.034178   \n",
       "2              -0.014867          -0.079175             0.244142   \n",
       "3              -0.061687          -0.031458             0.066656   \n",
       "4              -0.074735          -0.009163             0.057059   \n",
       "...                  ...                ...                  ...   \n",
       "12750           0.000251        -999.000000             0.012369   \n",
       "12751          -0.015458           0.005838             0.029858   \n",
       "12752          -0.007830           0.003689             0.119245   \n",
       "12753          -0.021534          -0.015512             0.039020   \n",
       "12754          -0.060736          -0.047039             0.049107   \n",
       "\n",
       "       STDEV_V_VELOCITY_02  STDEV_V_VELOCITY_03  STDEV_V_VELOCITY_04  \n",
       "0                 0.014972          -999.000000          -999.000000  \n",
       "1                 0.022088             0.040056             0.027100  \n",
       "2                 0.187899             0.136900             0.143455  \n",
       "3                 0.040829             0.068074             0.050827  \n",
       "4                 0.039952             0.046266             0.029428  \n",
       "...                    ...                  ...                  ...  \n",
       "12750             0.008401          -999.000000          -999.000000  \n",
       "12751             0.026927             0.016806             0.007840  \n",
       "12752             0.115518             0.108240             0.098283  \n",
       "12753             0.034755             0.028141             0.022072  \n",
       "12754             0.044644             0.036002             0.035390  \n",
       "\n",
       "[12755 rows x 72 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#============================================================\n",
    "# 기상 데이터 읽어오기 \n",
    "#============================================================\n",
    "ASOS = pd.read_csv(currentPath + \"/input/ASOS_imput.csv\", encoding='UTF-8') #loading weather data\n",
    "BUOY_DP = pd.read_csv(currentPath + \"/input/BUOY_DP_imput.csv\", encoding='UTF-8') #loading weather data\n",
    "HYCOM = pd.read_csv(currentPath + \"/input/SEA_IVs_hycom_all.csv\", encoding='UTF-8') #loading hycom data\n",
    "\n",
    "ASOS = ASOS.rename(columns = {\"WS_MAX\":\"WS_MAX_ASOS\"})\n",
    "BUOY_DP = BUOY_DP.rename(columns = {\"WS_MAX\":\"WS_MAX_BD\"})\n",
    "\n",
    "#=============================================================\n",
    "# 불러온 데이터 구조 확인하기\n",
    "#=============================================================\n",
    "ASOS\n",
    "BUOY_DP\n",
    "HYCOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['YYMMDD', 'HAEGU_NUM', 'year', 'month', 'AVG_EMP', 'MAX_SSH',\n",
       "       'AVG_SURFACE_SALINITY_TREND', 'STDEV_SURFACE_TEMPERATURE_TREND',\n",
       "       'AVG_SALINITY_01', 'AVG_SALINITY_02', 'AVG_SALINITY_03',\n",
       "       'AVG_SALINITY_04', 'MAX_SALINITY_01', 'MAX_SALINITY_02',\n",
       "       'MAX_SALINITY_03', 'MAX_SALINITY_04', 'MIN_SALINITY_01',\n",
       "       'MIN_SALINITY_02', 'MIN_SALINITY_03', 'MIN_SALINITY_04',\n",
       "       'STDEV_SALINITY_01', 'STDEV_SALINITY_02', 'STDEV_SALINITY_03',\n",
       "       'STDEV_SALINITY_04', 'AVG_TEMP_01', 'AVG_TEMP_02', 'AVG_TEMP_03',\n",
       "       'AVG_TEMP_04', 'MAX_TEMP_01', 'MAX_TEMP_02', 'MAX_TEMP_03',\n",
       "       'MAX_TEMP_04', 'MIN_TEMP_01', 'MIN_TEMP_02', 'MIN_TEMP_03',\n",
       "       'MIN_TEMP_04', 'STDEV_TEMP_01', 'STDEV_TEMP_02', 'STDEV_TEMP_03',\n",
       "       'STDEV_TEMP_04', 'AVG_U_VELOCITY_01', 'AVG_U_VELOCITY_02',\n",
       "       'AVG_U_VELOCITY_03', 'AVG_U_VELOCITY_04', 'MAX_U_VELOCITY_01',\n",
       "       'MAX_U_VELOCITY_02', 'MAX_U_VELOCITY_03', 'MAX_U_VELOCITY_04',\n",
       "       'MIN_U_VELOCITY_01', 'MIN_U_VELOCITY_02', 'MIN_U_VELOCITY_03',\n",
       "       'MIN_U_VELOCITY_04', 'STDEV_U_VELOCITY_01', 'STDEV_U_VELOCITY_02',\n",
       "       'STDEV_U_VELOCITY_03', 'STDEV_U_VELOCITY_04', 'AVG_V_VELOCITY_01',\n",
       "       'AVG_V_VELOCITY_02', 'AVG_V_VELOCITY_03', 'AVG_V_VELOCITY_04',\n",
       "       'MAX_V_VELOCITY_01', 'MAX_V_VELOCITY_02', 'MAX_V_VELOCITY_03',\n",
       "       'MAX_V_VELOCITY_04', 'MIN_V_VELOCITY_01', 'MIN_V_VELOCITY_02',\n",
       "       'MIN_V_VELOCITY_03', 'MIN_V_VELOCITY_04', 'STDEV_V_VELOCITY_01',\n",
       "       'STDEV_V_VELOCITY_02', 'STDEV_V_VELOCITY_03',\n",
       "       'STDEV_V_VELOCITY_04', 'WS_MAX_ASOS', 'WS_INS', 'TA_MAX', 'HM_AVG',\n",
       "       'PA_AVG', 'SS_DAY', 'EV_L', 'SI_DAY', 'RN_DAY', 'RN_DUR',\n",
       "       'WS_MAX_BD', 'WS_MIN', 'PS_MIN', 'WH_MAX'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=============================================================\n",
    "# 테이블 결합 및 확인 \n",
    "#=============================================================\n",
    "HYCOM['YYMMDD'] = pd.to_datetime(HYCOM['YYMMDD'], format='%Y-%m-%d')\n",
    "HYCOM[['year', 'month']] = HYCOM[['year', 'month']].apply(pd.to_numeric)\n",
    "HYCOM.dtypes\n",
    "\n",
    "ASOS['YYMMDD'] = pd.to_datetime(ASOS['YYMMDD'], format='%Y-%m-%d')\n",
    "ASOS[['year', 'month']] = ASOS[['year', 'month']].apply(pd.to_numeric)\n",
    "ASOS.dtypes\n",
    "\n",
    "BUOY_DP['YYMMDD'] = pd.to_datetime(BUOY_DP['YYMMDD'], format='%Y-%m-%d')\n",
    "BUOY_DP[['year', 'month']] = BUOY_DP[['year', 'month']].apply(pd.to_numeric)\n",
    "BUOY_DP.dtypes\n",
    "\n",
    "DT = pd.merge(HYCOM, ASOS, how='left', on=['HAEGU_NUM', 'YYMMDD', 'year', 'month'])\n",
    "DT = pd.merge(DT, BUOY_DP, how='left', on=['HAEGU_NUM', 'YYMMDD', 'year', 'month'])\n",
    "\n",
    "DT.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2324 entries, 0 to 2323\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   YYMMDD     2324 non-null   datetime64[ns]\n",
      " 1   Cochlo_YN  2324 non-null   int64         \n",
      " 2   type       1697 non-null   object        \n",
      " 3   Cell_min   2303 non-null   float64       \n",
      " 4   Cell_max   2262 non-null   float64       \n",
      " 5   LAT_r      2324 non-null   float64       \n",
      " 6   LON_r      2324 non-null   float64       \n",
      " 7   month      2324 non-null   int64         \n",
      " 8   year       2324 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(4), int64(3), object(1)\n",
      "memory usage: 163.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# 적조 데이터 읽어오기 \n",
    "#============================================================\n",
    "redtide = pd.read_csv(currentPath + \"/input/redtide.csv\", sep='\\t', encoding='UTF-8') #loading redtide data\n",
    "\n",
    "# column 수정\n",
    "redtide.rename(columns={redtide.columns[0]:\"YYMMDD\"}, inplace = True)\n",
    "redtide.rename(columns={redtide.columns[5]:\"LAT_r\"}, inplace = True)\n",
    "redtide.rename(columns={redtide.columns[6]:\"LON_r\"}, inplace = True)\n",
    "\n",
    "redtide['YYMMDD'] = pd.to_datetime(redtide['YYMMDD'], format='%Y%m%d')\n",
    "redtide['month'] = pd.to_numeric(redtide['YYMMDD'].dt.month)\n",
    "redtide['year'] = pd.to_numeric(redtide['YYMMDD'].dt.year)\n",
    "\n",
    "#============================================================\n",
    "# 불러온 데이터 구조 확인하기 \n",
    "#============================================================\n",
    "redtide.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1318 entries, 0 to 1317\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   HAEGU_NUM  1318 non-null   int64  \n",
      " 1   LAT_h      1318 non-null   float64\n",
      " 2   LON_h      1318 non-null   float64\n",
      " 3   row_h      1318 non-null   int64  \n",
      "dtypes: float64(2), int64(2)\n",
      "memory usage: 51.5 KB\n"
     ]
    }
   ],
   "source": [
    "#=============================================================\n",
    "# 해구 데이터 읽어오기\n",
    "#=============================================================\n",
    "HAEGU = pd.read_csv(currentPath + \"/input/SEA_latlon.csv\", sep='\\t', encoding='UTF-8') #loading redtide data\n",
    "HAEGU = HAEGU.sort_values(by=['HAEGU_NUM'])\n",
    "HAEGU['row_h'] = HAEGU.index + 1\n",
    "\n",
    "HAEGU.rename(columns={HAEGU.columns[1]:\"LAT_h\"}, inplace = True)\n",
    "HAEGU.rename(columns={HAEGU.columns[2]:\"LON_h\"}, inplace = True)\n",
    "\n",
    "#============================================================\n",
    "# 불러온 데이터 구조 확인하기 \n",
    "#============================================================\n",
    "HAEGU.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2324 entries, 0 to 2323\n",
      "Data columns (total 13 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   YYMMDD     2324 non-null   datetime64[ns]\n",
      " 1   Cochlo_YN  2324 non-null   int64         \n",
      " 2   type       1697 non-null   object        \n",
      " 3   Cell_min   2303 non-null   float64       \n",
      " 4   Cell_max   2262 non-null   float64       \n",
      " 5   LAT_r      2324 non-null   float64       \n",
      " 6   LON_r      2324 non-null   float64       \n",
      " 7   month      2324 non-null   int64         \n",
      " 8   year       2324 non-null   int64         \n",
      " 9   HAEGU_NUM  2324 non-null   int64         \n",
      " 10  LAT_h      2324 non-null   float64       \n",
      " 11  LON_h      2324 non-null   float64       \n",
      " 12  dist       2324 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(7), int64(4), object(1)\n",
      "memory usage: 254.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HAEGU_NUM</th>\n",
       "      <th>YYMMDD</th>\n",
       "      <th>Cochlo_YN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97</td>\n",
       "      <td>2009-08-17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>2009-10-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97</td>\n",
       "      <td>2009-10-30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97</td>\n",
       "      <td>2010-07-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97</td>\n",
       "      <td>2010-07-21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HAEGU_NUM     YYMMDD  Cochlo_YN\n",
       "0         97 2009-08-17          0\n",
       "1         97 2009-10-28          1\n",
       "2         97 2009-10-30          1\n",
       "3         97 2010-07-19          0\n",
       "4         97 2010-07-21          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=============================================================\n",
    "# 테이블 결합 및 확인\n",
    "#=============================================================\n",
    "match=redtide[[\"LAT_r\",\"LON_r\"]].drop_duplicates()\n",
    "match[\"row_r\"]=match.index+1\n",
    "\n",
    "def expand_grid(data_dict):\n",
    "    rows = itertools.product(*data_dict.values())\n",
    "    return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n",
    "tmp = expand_grid({'row_h' : HAEGU['row_h'], 'row_r' : match['row_r']})\n",
    "np.shape(tmp)[0]# nrow(match)*nrow(HAEGU)= 668*1318\n",
    "\n",
    "tmp = pd.merge(tmp, HAEGU, how='left', on=['row_h'])\n",
    "tmp = pd.merge(tmp, match, how='left', on=['row_r'])\n",
    "\n",
    "tmp['dist'] = np.hypot(tmp['LAT_h'].sub(tmp['LAT_r']), tmp['LON_h'].sub(tmp['LON_r']))\n",
    "\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "tmp = pysqldf(\"select HAEGU_NUM, LAT_h, LON_h, LAT_r, LON_r, min(dist) as dist from tmp group by row_r;\")\n",
    "redtide = pd.merge(redtide, tmp, how='left', on=['LAT_r','LON_r'])\n",
    "              \n",
    "redtide.info()\n",
    "redtide.head(5)\n",
    "\n",
    "#=============================================================\n",
    "# HAEGU, period 선택\n",
    "#=============================================================\n",
    "#start = datetime.datetime.strptime(\"2008-09-19\", \"%Y-%m-%d\").date()\n",
    "#end = datetime.datetime.strptime(\"2016-12-31\", \"%Y-%m-%d\").date()\n",
    "start = np.datetime64(\"2008-09-19\")\n",
    "end = np.datetime64(\"2016-12-31\")\n",
    "\n",
    "redtide = redtide.loc[pd.to_datetime(redtide['YYMMDD']) >= start]\n",
    "redtide = redtide.loc[pd.to_datetime(redtide['YYMMDD']) <= end]\n",
    "\n",
    "redtide = redtide[redtide['month'].isin([5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "redtide = redtide[redtide['HAEGU_NUM'].isin([97, 98, 99, 213, 214])]\n",
    "\n",
    "# 해구Num, 같은 날짜 여러 상태의 경우 Cochlo_YN값 MAX로 표출\n",
    "redtide = redtide.groupby(['HAEGU_NUM', 'YYMMDD']).max()['Cochlo_YN'].reset_index()\n",
    "redtide.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YYMMDD       0\n",
       "HAEGU_NUM    0\n",
       "year         0\n",
       "month        0\n",
       "AVG_EMP      0\n",
       "            ..\n",
       "WS_MAX_BD    0\n",
       "WS_MIN       0\n",
       "PS_MIN       0\n",
       "WH_MAX       0\n",
       "Cochlo_YN    0\n",
       "Length: 87, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=============================================================\n",
    "# 기상 데이터, 적조 데이터 결합\n",
    "#=============================================================\n",
    "AB = pd.merge(DT, redtide, how='left', on=['YYMMDD', 'HAEGU_NUM'])\n",
    "AB['Cochlo_YN'] = AB['Cochlo_YN'].fillna(0)\n",
    "\n",
    "# 데이터 결측치 확인\n",
    "AB.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================\n",
    "# 데이터 전처리\n",
    "#=============================================================\n",
    "AB.describe()\n",
    "\n",
    "# 기상변수에서 NA값이 -999로 처리된 경우 확인\n",
    "AB[AB==-999]=np.nan\n",
    "\n",
    "# 결측치 처리\n",
    "AB = AB.fillna(AB.mean(numeric_only=True))\n",
    "\n",
    "# 결측치 확인\n",
    "AB.isnull().sum()\n",
    "\n",
    "AB = AB.sort_values(by=['HAEGU_NUM', 'year', 'YYMMDD'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:5: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_EMP'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_EMP'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:6: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_SSH'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_SSH'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:7: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_SURFACE_SALINITY_TREND'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_SURFACE_SALINITY_TREND'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:8: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_SURFACE_TEMPERATURE_TREND'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SURFACE_TEMPERATURE_TREND'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:9: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_SALINITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_SALINITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:10: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_SALINITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_SALINITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_SALINITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_SALINITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:12: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_SALINITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_SALINITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:13: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_SALINITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_SALINITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:14: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_SALINITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_SALINITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:15: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_SALINITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_SALINITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:16: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_SALINITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_SALINITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:17: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_SALINITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_SALINITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:18: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_SALINITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_SALINITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_SALINITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_SALINITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:19: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_SALINITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_SALINITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_SALINITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_SALINITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:20: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_SALINITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_SALINITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_SALINITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_SALINITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:21: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_SALINITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SALINITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_SALINITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SALINITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:22: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_SALINITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SALINITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_SALINITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SALINITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:23: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_SALINITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SALINITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_SALINITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SALINITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:24: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_SALINITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SALINITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_SALINITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SALINITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:25: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_TEMP_01'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_TEMP_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_AVG_TEMP_01'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_TEMP_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:26: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_TEMP_02'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_TEMP_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_AVG_TEMP_02'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_TEMP_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:27: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_TEMP_03'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_TEMP_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_AVG_TEMP_03'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_TEMP_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:28: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_TEMP_04'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_TEMP_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_AVG_TEMP_04'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_TEMP_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:29: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_TEMP_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_TEMP_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MAX_TEMP_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_TEMP_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_TEMP_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_TEMP_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MAX_TEMP_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_TEMP_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:31: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_TEMP_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_TEMP_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MAX_TEMP_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_TEMP_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:32: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_TEMP_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_TEMP_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MAX_TEMP_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_TEMP_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:33: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_TEMP_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_TEMP_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_TEMP_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_TEMP_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:34: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_TEMP_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_TEMP_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_TEMP_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_TEMP_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_TEMP_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_TEMP_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_TEMP_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_TEMP_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:36: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_TEMP_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_TEMP_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_TEMP_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_TEMP_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:37: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_TEMP_01'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_TEMP_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_TEMP_01'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_TEMP_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:38: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_TEMP_02'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_TEMP_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_TEMP_02'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_TEMP_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:39: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_TEMP_03'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_TEMP_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_TEMP_03'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_TEMP_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:40: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_TEMP_04'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_TEMP_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_TEMP_04'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_TEMP_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:41: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_U_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_U_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_AVG_U_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_U_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:42: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_U_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_U_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_AVG_U_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_U_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:43: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_U_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_U_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_AVG_U_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_U_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:44: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_U_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_U_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_AVG_U_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_U_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:45: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_U_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_U_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MAX_U_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_U_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:46: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_U_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_U_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MAX_U_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_U_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:47: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_U_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_U_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MAX_U_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_U_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:48: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_U_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_U_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MAX_U_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_U_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:49: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_U_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_U_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_U_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_U_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:50: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_U_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_U_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_U_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_U_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:51: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_U_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_U_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_U_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_U_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:52: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_U_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_U_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_U_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_U_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:53: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_U_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_U_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_U_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_U_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:54: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_U_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_U_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_U_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_U_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:55: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_U_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_U_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_U_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_U_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_U_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_U_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_U_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_U_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:57: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_V_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_V_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_AVG_V_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_V_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:58: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_V_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_V_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_AVG_V_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_V_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:59: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_V_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_V_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_AVG_V_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_V_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:60: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_AVG_V_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_V_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_AVG_V_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_V_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_V_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_V_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MAX_V_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_V_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:62: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_V_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_V_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MAX_V_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_V_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:63: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_V_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_V_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MAX_V_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_V_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:64: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MAX_V_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_V_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MAX_V_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_V_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:65: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_V_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_V_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_V_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_V_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:66: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_V_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_V_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_V_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_V_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:67: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_V_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_V_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_V_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_V_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:68: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_MIN_V_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_V_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_MIN_V_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_V_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:69: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_V_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_V_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_V_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_V_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:70: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_V_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_V_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_V_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_V_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:71: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_V_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_V_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_V_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_V_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:72: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_STDEV_V_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_V_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\4227084244.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  AB['mean_STDEV_V_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_V_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n"
     ]
    }
   ],
   "source": [
    "#=============================================================\n",
    "# 파생변수 생성\n",
    "#=============================================================\n",
    "# 7일 평균값을 구함\n",
    "AB['mean_AVG_EMP'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_EMP'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_SSH'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_SSH'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_SURFACE_SALINITY_TREND'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_SURFACE_SALINITY_TREND'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_SURFACE_TEMPERATURE_TREND'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SURFACE_TEMPERATURE_TREND'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_SALINITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_SALINITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_SALINITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_SALINITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_SALINITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_SALINITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_SALINITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_SALINITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_SALINITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_SALINITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_SALINITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_SALINITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_SALINITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_SALINITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_SALINITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_SALINITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_SALINITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_SALINITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_SALINITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_SALINITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_SALINITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_SALINITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_SALINITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_SALINITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_SALINITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SALINITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_SALINITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SALINITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_SALINITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SALINITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_SALINITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_SALINITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_TEMP_01'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_TEMP_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_TEMP_02'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_TEMP_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_TEMP_03'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_TEMP_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_TEMP_04'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_TEMP_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_TEMP_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_TEMP_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_TEMP_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_TEMP_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_TEMP_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_TEMP_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_TEMP_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_TEMP_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_TEMP_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_TEMP_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_TEMP_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_TEMP_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_TEMP_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_TEMP_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_TEMP_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_TEMP_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_TEMP_01'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_TEMP_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_TEMP_02'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_TEMP_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_TEMP_03'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_TEMP_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_TEMP_04'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_TEMP_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_U_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_U_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_U_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_U_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_U_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_U_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_U_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_U_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_U_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_U_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_U_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_U_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_U_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_U_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_U_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_U_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_U_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_U_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_U_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_U_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_U_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_U_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_U_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_U_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_U_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_U_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_U_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_U_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_U_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_U_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_U_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_U_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_V_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_V_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_V_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_V_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_V_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_V_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_AVG_V_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['AVG_V_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_V_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_V_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_V_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_V_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_V_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_V_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MAX_V_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MAX_V_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_V_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_V_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_V_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_V_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_V_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_V_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_MIN_V_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['MIN_V_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_V_VELOCITY_01'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_V_VELOCITY_01'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_V_VELOCITY_02'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_V_VELOCITY_02'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_V_VELOCITY_03'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_V_VELOCITY_03'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "AB['mean_STDEV_V_VELOCITY_04'] = AB.groupby(['HAEGU_NUM', 'year'])['STDEV_V_VELOCITY_04'].apply(lambda x : x.rolling(7).sum().shift(1)) / 7\n",
    "\n",
    "# 전처리 이전 변수 삭제\n",
    "AB.drop(['AVG_EMP', 'MAX_SSH', 'AVG_SURFACE_SALINITY_TREND', 'STDEV_SURFACE_TEMPERATURE_TREND', 'AVG_SALINITY_01', 'AVG_SALINITY_02'\n",
    ", 'AVG_SALINITY_03', 'AVG_SALINITY_04', 'MAX_SALINITY_01', 'MAX_SALINITY_02', 'MAX_SALINITY_03', 'MAX_SALINITY_04', 'MIN_SALINITY_01'\n",
    ", 'MIN_SALINITY_02', 'MIN_SALINITY_03', 'MIN_SALINITY_04', 'STDEV_SALINITY_01', 'STDEV_SALINITY_02', 'STDEV_SALINITY_03'\n",
    ", 'STDEV_SALINITY_04', 'AVG_TEMP_01', 'AVG_TEMP_02', 'AVG_TEMP_03', 'AVG_TEMP_04', 'MAX_TEMP_01', 'MAX_TEMP_02', 'MAX_TEMP_03'\n",
    ", 'MAX_TEMP_04', 'MIN_TEMP_01', 'MIN_TEMP_02', 'MIN_TEMP_03', 'MIN_TEMP_04', 'STDEV_TEMP_01', 'STDEV_TEMP_02', 'STDEV_TEMP_03'\n",
    ", 'STDEV_TEMP_04', 'AVG_U_VELOCITY_01', 'AVG_U_VELOCITY_02', 'AVG_U_VELOCITY_03', 'AVG_U_VELOCITY_04', 'MAX_U_VELOCITY_01'\n",
    ", 'MAX_U_VELOCITY_02', 'MAX_U_VELOCITY_03', 'MAX_U_VELOCITY_04', 'MIN_U_VELOCITY_01', 'MIN_U_VELOCITY_02', 'MIN_U_VELOCITY_03'\n",
    ", 'MIN_U_VELOCITY_04', 'STDEV_U_VELOCITY_01', 'STDEV_U_VELOCITY_02', 'STDEV_U_VELOCITY_03', 'STDEV_U_VELOCITY_04'\n",
    ", 'AVG_V_VELOCITY_01', 'AVG_V_VELOCITY_02', 'AVG_V_VELOCITY_03', 'AVG_V_VELOCITY_04', 'MAX_V_VELOCITY_01', 'MAX_V_VELOCITY_02'\n",
    ", 'MAX_V_VELOCITY_03', 'MAX_V_VELOCITY_04', 'MIN_V_VELOCITY_01', 'MIN_V_VELOCITY_02', 'MIN_V_VELOCITY_03', 'MIN_V_VELOCITY_04'\n",
    ", 'STDEV_V_VELOCITY_01', 'STDEV_V_VELOCITY_02', 'STDEV_V_VELOCITY_03', 'STDEV_V_VELOCITY_04'], axis='columns', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:2: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_TA_MAX'] = AB.groupby(['HAEGU_NUM', 'year'])['TA_MAX'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:3: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_WS_MAX_ASOS'] = AB.groupby(['HAEGU_NUM', 'year'])['WS_MAX_ASOS'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:4: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_WS_MAX_BD'] = AB.groupby(['HAEGU_NUM', 'year'])['WS_MAX_BD'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:5: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_WS_INS'] = AB.groupby(['HAEGU_NUM', 'year'])['WS_INS'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:6: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_WS_MIN'] = AB.groupby(['HAEGU_NUM', 'year'])['WS_MIN'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:7: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_HM_AVG'] = AB.groupby(['HAEGU_NUM', 'year'])['HM_AVG'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:8: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_EV_L'] = AB.groupby(['HAEGU_NUM', 'year'])['EV_L'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:9: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_PA_AVG'] = AB.groupby(['HAEGU_NUM', 'year'])['PA_AVG'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:10: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_PS_MIN'] = AB.groupby(['HAEGU_NUM', 'year'])['PS_MIN'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['mean_WH_MAX'] = AB.groupby(['HAEGU_NUM', 'year'])['WH_MAX'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:17: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['sum_SS_DAY'] = AB.groupby(['HAEGU_NUM', 'year'])['SS_DAY'].apply(lambda x : x.rolling(14).sum().shift(1))\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:18: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['sum_RN_DAY'] = AB.groupby(['HAEGU_NUM', 'year'])['RN_DAY'].apply(lambda x : x.rolling(14).sum().shift(1))\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:19: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['sum_SI_DAY'] = AB.groupby(['HAEGU_NUM', 'year'])['SI_DAY'].apply(lambda x : x.rolling(14).sum().shift(1))\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19432\\2688162299.py:20: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  AB['sum_RN_DUR'] = AB.groupby(['HAEGU_NUM', 'year'])['RN_DUR'].apply(lambda x : x.rolling(14).sum().shift(1))\n"
     ]
    }
   ],
   "source": [
    "# 14일 평균값을 구함\n",
    "AB['mean_TA_MAX'] = AB.groupby(['HAEGU_NUM', 'year'])['TA_MAX'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
    "AB['mean_WS_MAX_ASOS'] = AB.groupby(['HAEGU_NUM', 'year'])['WS_MAX_ASOS'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
    "AB['mean_WS_MAX_BD'] = AB.groupby(['HAEGU_NUM', 'year'])['WS_MAX_BD'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
    "AB['mean_WS_INS'] = AB.groupby(['HAEGU_NUM', 'year'])['WS_INS'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
    "AB['mean_WS_MIN'] = AB.groupby(['HAEGU_NUM', 'year'])['WS_MIN'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
    "AB['mean_HM_AVG'] = AB.groupby(['HAEGU_NUM', 'year'])['HM_AVG'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
    "AB['mean_EV_L'] = AB.groupby(['HAEGU_NUM', 'year'])['EV_L'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
    "AB['mean_PA_AVG'] = AB.groupby(['HAEGU_NUM', 'year'])['PA_AVG'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
    "AB['mean_PS_MIN'] = AB.groupby(['HAEGU_NUM', 'year'])['PS_MIN'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
    "AB['mean_WH_MAX'] = AB.groupby(['HAEGU_NUM', 'year'])['WH_MAX'].apply(lambda x : x.rolling(14).sum().shift(1)) / 14\n",
    "\n",
    "# 전처리 이전 변수 삭제\n",
    "AB.drop(['TA_MAX', 'WS_MAX_ASOS', 'WS_MAX_BD', 'WS_INS', 'WS_MIN', 'HM_AVG', 'EV_L', 'PA_AVG', 'PS_MIN', 'WH_MAX'], axis='columns', inplace=True)\n",
    "\n",
    "# 14일 누적치를 구함\n",
    "AB['sum_SS_DAY'] = AB.groupby(['HAEGU_NUM', 'year'])['SS_DAY'].apply(lambda x : x.rolling(14).sum().shift(1))\n",
    "AB['sum_RN_DAY'] = AB.groupby(['HAEGU_NUM', 'year'])['RN_DAY'].apply(lambda x : x.rolling(14).sum().shift(1))\n",
    "AB['sum_SI_DAY'] = AB.groupby(['HAEGU_NUM', 'year'])['SI_DAY'].apply(lambda x : x.rolling(14).sum().shift(1))\n",
    "AB['sum_RN_DUR'] = AB.groupby(['HAEGU_NUM', 'year'])['RN_DUR'].apply(lambda x : x.rolling(14).sum().shift(1))\n",
    "AB.drop(['SS_DAY', 'RN_DAY', 'SI_DAY', 'RN_DUR'], axis='columns', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8850 entries, 70 to 12599\n",
      "Data columns (total 87 columns):\n",
      " #   Column                                Non-Null Count  Dtype         \n",
      "---  ------                                --------------  -----         \n",
      " 0   YYMMDD                                8850 non-null   datetime64[ns]\n",
      " 1   HAEGU_NUM                             8850 non-null   int64         \n",
      " 2   year                                  8850 non-null   int64         \n",
      " 3   month                                 8850 non-null   int64         \n",
      " 4   Cochlo_YN                             8850 non-null   float64       \n",
      " 5   mean_AVG_EMP                          8850 non-null   float64       \n",
      " 6   mean_MAX_SSH                          8850 non-null   float64       \n",
      " 7   mean_AVG_SURFACE_SALINITY_TREND       8850 non-null   float64       \n",
      " 8   mean_STDEV_SURFACE_TEMPERATURE_TREND  8850 non-null   float64       \n",
      " 9   mean_AVG_SALINITY_01                  8850 non-null   float64       \n",
      " 10  mean_AVG_SALINITY_02                  8850 non-null   float64       \n",
      " 11  mean_AVG_SALINITY_03                  8850 non-null   float64       \n",
      " 12  mean_AVG_SALINITY_04                  8850 non-null   float64       \n",
      " 13  mean_MAX_SALINITY_01                  8850 non-null   float64       \n",
      " 14  mean_MAX_SALINITY_02                  8850 non-null   float64       \n",
      " 15  mean_MAX_SALINITY_03                  8850 non-null   float64       \n",
      " 16  mean_MAX_SALINITY_04                  8850 non-null   float64       \n",
      " 17  mean_MIN_SALINITY_01                  8850 non-null   float64       \n",
      " 18  mean_MIN_SALINITY_02                  8850 non-null   float64       \n",
      " 19  mean_MIN_SALINITY_03                  8850 non-null   float64       \n",
      " 20  mean_MIN_SALINITY_04                  8850 non-null   float64       \n",
      " 21  mean_STDEV_SALINITY_01                8850 non-null   float64       \n",
      " 22  mean_STDEV_SALINITY_02                8850 non-null   float64       \n",
      " 23  mean_STDEV_SALINITY_03                8850 non-null   float64       \n",
      " 24  mean_STDEV_SALINITY_04                8850 non-null   float64       \n",
      " 25  mean_AVG_TEMP_01                      8850 non-null   float64       \n",
      " 26  mean_AVG_TEMP_02                      8850 non-null   float64       \n",
      " 27  mean_AVG_TEMP_03                      8850 non-null   float64       \n",
      " 28  mean_AVG_TEMP_04                      8850 non-null   float64       \n",
      " 29  mean_MAX_TEMP_01                      8850 non-null   float64       \n",
      " 30  mean_MAX_TEMP_02                      8850 non-null   float64       \n",
      " 31  mean_MAX_TEMP_03                      8850 non-null   float64       \n",
      " 32  mean_MAX_TEMP_04                      8850 non-null   float64       \n",
      " 33  mean_MIN_TEMP_01                      8850 non-null   float64       \n",
      " 34  mean_MIN_TEMP_02                      8850 non-null   float64       \n",
      " 35  mean_MIN_TEMP_03                      8850 non-null   float64       \n",
      " 36  mean_MIN_TEMP_04                      8850 non-null   float64       \n",
      " 37  mean_STDEV_TEMP_01                    8850 non-null   float64       \n",
      " 38  mean_STDEV_TEMP_02                    8850 non-null   float64       \n",
      " 39  mean_STDEV_TEMP_03                    8850 non-null   float64       \n",
      " 40  mean_STDEV_TEMP_04                    8850 non-null   float64       \n",
      " 41  mean_AVG_U_VELOCITY_01                8850 non-null   float64       \n",
      " 42  mean_AVG_U_VELOCITY_02                8850 non-null   float64       \n",
      " 43  mean_AVG_U_VELOCITY_03                8850 non-null   float64       \n",
      " 44  mean_AVG_U_VELOCITY_04                8850 non-null   float64       \n",
      " 45  mean_MAX_U_VELOCITY_01                8850 non-null   float64       \n",
      " 46  mean_MAX_U_VELOCITY_02                8850 non-null   float64       \n",
      " 47  mean_MAX_U_VELOCITY_03                8850 non-null   float64       \n",
      " 48  mean_MAX_U_VELOCITY_04                8850 non-null   float64       \n",
      " 49  mean_MIN_U_VELOCITY_01                8850 non-null   float64       \n",
      " 50  mean_MIN_U_VELOCITY_02                8850 non-null   float64       \n",
      " 51  mean_MIN_U_VELOCITY_03                8850 non-null   float64       \n",
      " 52  mean_MIN_U_VELOCITY_04                8850 non-null   float64       \n",
      " 53  mean_STDEV_U_VELOCITY_01              8850 non-null   float64       \n",
      " 54  mean_STDEV_U_VELOCITY_02              8850 non-null   float64       \n",
      " 55  mean_STDEV_U_VELOCITY_03              8850 non-null   float64       \n",
      " 56  mean_STDEV_U_VELOCITY_04              8850 non-null   float64       \n",
      " 57  mean_AVG_V_VELOCITY_01                8850 non-null   float64       \n",
      " 58  mean_AVG_V_VELOCITY_02                8850 non-null   float64       \n",
      " 59  mean_AVG_V_VELOCITY_03                8850 non-null   float64       \n",
      " 60  mean_AVG_V_VELOCITY_04                8850 non-null   float64       \n",
      " 61  mean_MAX_V_VELOCITY_01                8850 non-null   float64       \n",
      " 62  mean_MAX_V_VELOCITY_02                8850 non-null   float64       \n",
      " 63  mean_MAX_V_VELOCITY_03                8850 non-null   float64       \n",
      " 64  mean_MAX_V_VELOCITY_04                8850 non-null   float64       \n",
      " 65  mean_MIN_V_VELOCITY_01                8850 non-null   float64       \n",
      " 66  mean_MIN_V_VELOCITY_02                8850 non-null   float64       \n",
      " 67  mean_MIN_V_VELOCITY_03                8850 non-null   float64       \n",
      " 68  mean_MIN_V_VELOCITY_04                8850 non-null   float64       \n",
      " 69  mean_STDEV_V_VELOCITY_01              8850 non-null   float64       \n",
      " 70  mean_STDEV_V_VELOCITY_02              8850 non-null   float64       \n",
      " 71  mean_STDEV_V_VELOCITY_03              8850 non-null   float64       \n",
      " 72  mean_STDEV_V_VELOCITY_04              8850 non-null   float64       \n",
      " 73  mean_TA_MAX                           8850 non-null   float64       \n",
      " 74  mean_WS_MAX_ASOS                      8850 non-null   float64       \n",
      " 75  mean_WS_MAX_BD                        8850 non-null   float64       \n",
      " 76  mean_WS_INS                           8850 non-null   float64       \n",
      " 77  mean_WS_MIN                           8850 non-null   float64       \n",
      " 78  mean_HM_AVG                           8850 non-null   float64       \n",
      " 79  mean_EV_L                             8850 non-null   float64       \n",
      " 80  mean_PA_AVG                           8850 non-null   float64       \n",
      " 81  mean_PS_MIN                           8850 non-null   float64       \n",
      " 82  mean_WH_MAX                           8850 non-null   float64       \n",
      " 83  sum_SS_DAY                            8850 non-null   float64       \n",
      " 84  sum_RN_DAY                            8850 non-null   float64       \n",
      " 85  sum_SI_DAY                            8850 non-null   float64       \n",
      " 86  sum_RN_DUR                            8850 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(83), int64(3)\n",
      "memory usage: 5.9 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YYMMDD</th>\n",
       "      <th>HAEGU_NUM</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>Cochlo_YN</th>\n",
       "      <th>mean_AVG_EMP</th>\n",
       "      <th>mean_MAX_SSH</th>\n",
       "      <th>mean_AVG_SURFACE_SALINITY_TREND</th>\n",
       "      <th>mean_STDEV_SURFACE_TEMPERATURE_TREND</th>\n",
       "      <th>mean_AVG_SALINITY_01</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_WS_MIN</th>\n",
       "      <th>mean_HM_AVG</th>\n",
       "      <th>mean_EV_L</th>\n",
       "      <th>mean_PA_AVG</th>\n",
       "      <th>mean_PS_MIN</th>\n",
       "      <th>mean_WH_MAX</th>\n",
       "      <th>sum_SS_DAY</th>\n",
       "      <th>sum_RN_DAY</th>\n",
       "      <th>sum_SI_DAY</th>\n",
       "      <th>sum_RN_DUR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2008-10-04</td>\n",
       "      <td>97</td>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>0.208562</td>\n",
       "      <td>0.079170</td>\n",
       "      <td>0.137183</td>\n",
       "      <td>31.689443</td>\n",
       "      <td>...</td>\n",
       "      <td>3.190673</td>\n",
       "      <td>66.689079</td>\n",
       "      <td>2.931534</td>\n",
       "      <td>1001.556916</td>\n",
       "      <td>1010.343008</td>\n",
       "      <td>3.358109</td>\n",
       "      <td>54.180084</td>\n",
       "      <td>21.338384</td>\n",
       "      <td>1.080137</td>\n",
       "      <td>8.321869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2008-10-05</td>\n",
       "      <td>97</td>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>0.201662</td>\n",
       "      <td>0.107633</td>\n",
       "      <td>0.216339</td>\n",
       "      <td>31.711071</td>\n",
       "      <td>...</td>\n",
       "      <td>3.100917</td>\n",
       "      <td>65.401030</td>\n",
       "      <td>2.908604</td>\n",
       "      <td>1001.814943</td>\n",
       "      <td>1010.726256</td>\n",
       "      <td>3.304844</td>\n",
       "      <td>57.397615</td>\n",
       "      <td>20.683448</td>\n",
       "      <td>1.097392</td>\n",
       "      <td>7.218716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2008-10-06</td>\n",
       "      <td>97</td>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>0.210660</td>\n",
       "      <td>0.106639</td>\n",
       "      <td>0.213024</td>\n",
       "      <td>31.730586</td>\n",
       "      <td>...</td>\n",
       "      <td>3.107218</td>\n",
       "      <td>65.120337</td>\n",
       "      <td>2.817988</td>\n",
       "      <td>1001.988595</td>\n",
       "      <td>1010.962944</td>\n",
       "      <td>2.762914</td>\n",
       "      <td>55.369954</td>\n",
       "      <td>1.101480</td>\n",
       "      <td>1.080196</td>\n",
       "      <td>3.840809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2008-10-07</td>\n",
       "      <td>97</td>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>0.200848</td>\n",
       "      <td>0.130929</td>\n",
       "      <td>0.261637</td>\n",
       "      <td>31.746943</td>\n",
       "      <td>...</td>\n",
       "      <td>3.037457</td>\n",
       "      <td>63.842112</td>\n",
       "      <td>2.868570</td>\n",
       "      <td>1002.129137</td>\n",
       "      <td>1011.180098</td>\n",
       "      <td>2.720642</td>\n",
       "      <td>52.816630</td>\n",
       "      <td>1.102438</td>\n",
       "      <td>1.051451</td>\n",
       "      <td>3.841749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2008-10-08</td>\n",
       "      <td>97</td>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>0.208403</td>\n",
       "      <td>0.159245</td>\n",
       "      <td>0.304827</td>\n",
       "      <td>31.778571</td>\n",
       "      <td>...</td>\n",
       "      <td>3.190235</td>\n",
       "      <td>62.286809</td>\n",
       "      <td>2.902726</td>\n",
       "      <td>1002.464811</td>\n",
       "      <td>1011.523714</td>\n",
       "      <td>2.703223</td>\n",
       "      <td>55.182198</td>\n",
       "      <td>0.891279</td>\n",
       "      <td>1.076749</td>\n",
       "      <td>3.822488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       YYMMDD  HAEGU_NUM  year  month  Cochlo_YN  mean_AVG_EMP  mean_MAX_SSH  \\\n",
       "70 2008-10-04         97  2008     10        0.0     -0.000148      0.208562   \n",
       "75 2008-10-05         97  2008     10        0.0     -0.000140      0.201662   \n",
       "80 2008-10-06         97  2008     10        0.0     -0.000136      0.210660   \n",
       "85 2008-10-07         97  2008     10        0.0     -0.000132      0.200848   \n",
       "90 2008-10-08         97  2008     10        0.0     -0.000137      0.208403   \n",
       "\n",
       "    mean_AVG_SURFACE_SALINITY_TREND  mean_STDEV_SURFACE_TEMPERATURE_TREND  \\\n",
       "70                         0.079170                              0.137183   \n",
       "75                         0.107633                              0.216339   \n",
       "80                         0.106639                              0.213024   \n",
       "85                         0.130929                              0.261637   \n",
       "90                         0.159245                              0.304827   \n",
       "\n",
       "    mean_AVG_SALINITY_01  ...  mean_WS_MIN  mean_HM_AVG  mean_EV_L  \\\n",
       "70             31.689443  ...     3.190673    66.689079   2.931534   \n",
       "75             31.711071  ...     3.100917    65.401030   2.908604   \n",
       "80             31.730586  ...     3.107218    65.120337   2.817988   \n",
       "85             31.746943  ...     3.037457    63.842112   2.868570   \n",
       "90             31.778571  ...     3.190235    62.286809   2.902726   \n",
       "\n",
       "    mean_PA_AVG  mean_PS_MIN  mean_WH_MAX  sum_SS_DAY  sum_RN_DAY  sum_SI_DAY  \\\n",
       "70  1001.556916  1010.343008     3.358109   54.180084   21.338384    1.080137   \n",
       "75  1001.814943  1010.726256     3.304844   57.397615   20.683448    1.097392   \n",
       "80  1001.988595  1010.962944     2.762914   55.369954    1.101480    1.080196   \n",
       "85  1002.129137  1011.180098     2.720642   52.816630    1.102438    1.051451   \n",
       "90  1002.464811  1011.523714     2.703223   55.182198    0.891279    1.076749   \n",
       "\n",
       "    sum_RN_DUR  \n",
       "70    8.321869  \n",
       "75    7.218716  \n",
       "80    3.840809  \n",
       "85    3.841749  \n",
       "90    3.822488  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time interval create\n",
    "AB['Cochlo_YN'] = AB.groupby(['HAEGU_NUM', 'year'])['Cochlo_YN'].shift(-7)\n",
    "\n",
    "#start = datetime.datetime.strptime(\"2008-10-04\", \"%Y-%m-%d\").date()\n",
    "#end = datetime.datetime.strptime(\"2016-12-31\", \"%Y-%m-%d\").date()\n",
    "start = np.datetime64(\"2008-10-04\")\n",
    "end = np.datetime64(\"2016-12-31\")\n",
    "\n",
    "AB = AB.loc[pd.to_datetime(AB['YYMMDD']) >= start]\n",
    "AB = AB.loc[pd.to_datetime(AB['YYMMDD']) <= end]\n",
    "\n",
    "AB = AB[AB['month'].isin([5, 6, 7, 8, 9, 10, 11])]\n",
    "\n",
    "#======================================================================================================\n",
    "#메모리 용량 줄이기\n",
    "#======================================================================================================\n",
    "del(start, end)\n",
    "del(ASOS,BUOY_DP,DT,HAEGU,HYCOM,redtide,tmp,match)\n",
    "\n",
    "AB.info()\n",
    "AB.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.19+9-LTS-224, mixed mode)\n",
      "  Starting server from C:\\Users\\user\\anaconda3\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\user\\AppData\\Local\\Temp\\tmprwvs103q\n",
      "  JVM stdout: C:\\Users\\user\\AppData\\Local\\Temp\\tmprwvs103q\\h2o_user_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\user\\AppData\\Local\\Temp\\tmprwvs103q\\h2o_user_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Asia/Seoul</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.42.0.1</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>23 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_user_qfefht</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>4 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.9 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         01 secs\n",
       "H2O_cluster_timezone:       Asia/Seoul\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.42.0.1\n",
       "H2O_cluster_version_age:    23 days\n",
       "H2O_cluster_name:           H2O_from_python_user_qfefht\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    4 Gb\n",
       "H2O_cluster_total_cores:    16\n",
       "H2O_cluster_allowed_cores:  1\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.9 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#=============================================================\n",
    "# 분석\n",
    "#=============================================================\n",
    "#2o.cluster().shutdown()\n",
    "h2o.init(max_mem_size = \"4G\", nthreads = 1, port=54321)\n",
    "#h2o.init()\n",
    "h2o.remove_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8850 entries, 0 to 8849\n",
      "Data columns (total 83 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   Y                                     8850 non-null   float64\n",
      " 1   mean_AVG_EMP                          8850 non-null   float64\n",
      " 2   mean_MAX_SSH                          8850 non-null   float64\n",
      " 3   mean_AVG_SURFACE_SALINITY_TREND       8850 non-null   float64\n",
      " 4   mean_STDEV_SURFACE_TEMPERATURE_TREND  8850 non-null   float64\n",
      " 5   mean_AVG_SALINITY_01                  8850 non-null   float64\n",
      " 6   mean_AVG_SALINITY_02                  8850 non-null   float64\n",
      " 7   mean_AVG_SALINITY_03                  8850 non-null   float64\n",
      " 8   mean_AVG_SALINITY_04                  8850 non-null   float64\n",
      " 9   mean_MAX_SALINITY_01                  8850 non-null   float64\n",
      " 10  mean_MAX_SALINITY_02                  8850 non-null   float64\n",
      " 11  mean_MAX_SALINITY_03                  8850 non-null   float64\n",
      " 12  mean_MAX_SALINITY_04                  8850 non-null   float64\n",
      " 13  mean_MIN_SALINITY_01                  8850 non-null   float64\n",
      " 14  mean_MIN_SALINITY_02                  8850 non-null   float64\n",
      " 15  mean_MIN_SALINITY_03                  8850 non-null   float64\n",
      " 16  mean_MIN_SALINITY_04                  8850 non-null   float64\n",
      " 17  mean_STDEV_SALINITY_01                8850 non-null   float64\n",
      " 18  mean_STDEV_SALINITY_02                8850 non-null   float64\n",
      " 19  mean_STDEV_SALINITY_03                8850 non-null   float64\n",
      " 20  mean_STDEV_SALINITY_04                8850 non-null   float64\n",
      " 21  mean_AVG_TEMP_01                      8850 non-null   float64\n",
      " 22  mean_AVG_TEMP_02                      8850 non-null   float64\n",
      " 23  mean_AVG_TEMP_03                      8850 non-null   float64\n",
      " 24  mean_AVG_TEMP_04                      8850 non-null   float64\n",
      " 25  mean_MAX_TEMP_01                      8850 non-null   float64\n",
      " 26  mean_MAX_TEMP_02                      8850 non-null   float64\n",
      " 27  mean_MAX_TEMP_03                      8850 non-null   float64\n",
      " 28  mean_MAX_TEMP_04                      8850 non-null   float64\n",
      " 29  mean_MIN_TEMP_01                      8850 non-null   float64\n",
      " 30  mean_MIN_TEMP_02                      8850 non-null   float64\n",
      " 31  mean_MIN_TEMP_03                      8850 non-null   float64\n",
      " 32  mean_MIN_TEMP_04                      8850 non-null   float64\n",
      " 33  mean_STDEV_TEMP_01                    8850 non-null   float64\n",
      " 34  mean_STDEV_TEMP_02                    8850 non-null   float64\n",
      " 35  mean_STDEV_TEMP_03                    8850 non-null   float64\n",
      " 36  mean_STDEV_TEMP_04                    8850 non-null   float64\n",
      " 37  mean_AVG_U_VELOCITY_01                8850 non-null   float64\n",
      " 38  mean_AVG_U_VELOCITY_02                8850 non-null   float64\n",
      " 39  mean_AVG_U_VELOCITY_03                8850 non-null   float64\n",
      " 40  mean_AVG_U_VELOCITY_04                8850 non-null   float64\n",
      " 41  mean_MAX_U_VELOCITY_01                8850 non-null   float64\n",
      " 42  mean_MAX_U_VELOCITY_02                8850 non-null   float64\n",
      " 43  mean_MAX_U_VELOCITY_03                8850 non-null   float64\n",
      " 44  mean_MAX_U_VELOCITY_04                8850 non-null   float64\n",
      " 45  mean_MIN_U_VELOCITY_01                8850 non-null   float64\n",
      " 46  mean_MIN_U_VELOCITY_02                8850 non-null   float64\n",
      " 47  mean_MIN_U_VELOCITY_03                8850 non-null   float64\n",
      " 48  mean_MIN_U_VELOCITY_04                8850 non-null   float64\n",
      " 49  mean_STDEV_U_VELOCITY_01              8850 non-null   float64\n",
      " 50  mean_STDEV_U_VELOCITY_02              8850 non-null   float64\n",
      " 51  mean_STDEV_U_VELOCITY_03              8850 non-null   float64\n",
      " 52  mean_STDEV_U_VELOCITY_04              8850 non-null   float64\n",
      " 53  mean_AVG_V_VELOCITY_01                8850 non-null   float64\n",
      " 54  mean_AVG_V_VELOCITY_02                8850 non-null   float64\n",
      " 55  mean_AVG_V_VELOCITY_03                8850 non-null   float64\n",
      " 56  mean_AVG_V_VELOCITY_04                8850 non-null   float64\n",
      " 57  mean_MAX_V_VELOCITY_01                8850 non-null   float64\n",
      " 58  mean_MAX_V_VELOCITY_02                8850 non-null   float64\n",
      " 59  mean_MAX_V_VELOCITY_03                8850 non-null   float64\n",
      " 60  mean_MAX_V_VELOCITY_04                8850 non-null   float64\n",
      " 61  mean_MIN_V_VELOCITY_01                8850 non-null   float64\n",
      " 62  mean_MIN_V_VELOCITY_02                8850 non-null   float64\n",
      " 63  mean_MIN_V_VELOCITY_03                8850 non-null   float64\n",
      " 64  mean_MIN_V_VELOCITY_04                8850 non-null   float64\n",
      " 65  mean_STDEV_V_VELOCITY_01              8850 non-null   float64\n",
      " 66  mean_STDEV_V_VELOCITY_02              8850 non-null   float64\n",
      " 67  mean_STDEV_V_VELOCITY_03              8850 non-null   float64\n",
      " 68  mean_STDEV_V_VELOCITY_04              8850 non-null   float64\n",
      " 69  mean_TA_MAX                           8850 non-null   float64\n",
      " 70  mean_WS_MAX_ASOS                      8850 non-null   float64\n",
      " 71  mean_WS_MAX_BD                        8850 non-null   float64\n",
      " 72  mean_WS_INS                           8850 non-null   float64\n",
      " 73  mean_WS_MIN                           8850 non-null   float64\n",
      " 74  mean_HM_AVG                           8850 non-null   float64\n",
      " 75  mean_EV_L                             8850 non-null   float64\n",
      " 76  mean_PA_AVG                           8850 non-null   float64\n",
      " 77  mean_PS_MIN                           8850 non-null   float64\n",
      " 78  mean_WH_MAX                           8850 non-null   float64\n",
      " 79  sum_SS_DAY                            8850 non-null   float64\n",
      " 80  sum_RN_DAY                            8850 non-null   float64\n",
      " 81  sum_SI_DAY                            8850 non-null   float64\n",
      " 82  sum_RN_DUR                            8850 non-null   float64\n",
      "dtypes: float64(83)\n",
      "memory usage: 5.6 MB\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "# setting AB data\n",
    "dataXY = AB\n",
    "dataXY = dataXY.drop(['HAEGU_NUM', 'YYMMDD', 'month', 'year'], axis=1)\n",
    "dataXY = dataXY.rename(columns = {\"Cochlo_YN\":\"Y\"})\n",
    "dataXY.reset_index(drop=True, inplace=True)\n",
    "dataXY.info()\n",
    "\n",
    "# train, valid, test 데이터 분리\n",
    "## split to train, valid, test \n",
    "dataXY = h2o.H2OFrame(dataXY)\n",
    "train_data, valid_data, test_data = dataXY.split_frame(ratios=[0.7,0.15], seed=1111)\n",
    "\n",
    "## 독립변수, 종속변수 설정(x: 독립변수, y: 종속변수)\n",
    "x = dataXY.columns\n",
    "x.remove('Y')\n",
    "y='Y'\n",
    "\n",
    "train_data['Y'] = train_data['Y'].asfactor()\n",
    "valid_data['Y'] = valid_data['Y'].asfactor()\n",
    "test_data['Y'] = test_data['Y'].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Grid Build progress: |███████████████████████████████████████████████████████| (done) 100%\n",
      "===== sortedGrid =====\n",
      "Hyper-Parameter Search Summary: ordered by decreasing auc\n",
      "    max_depth    model_ids              auc\n",
      "--  -----------  ---------------------  --------\n",
      "    12           RF_depth_grid_model_4  0.983323\n",
      "    8            RF_depth_grid_model_3  0.976329\n",
      "    16           RF_depth_grid_model_5  0.976284\n",
      "    20           RF_depth_grid_model_6  0.973914\n",
      "    6            RF_depth_grid_model_2  0.970839\n",
      "    4            RF_depth_grid_model_1  0.960837\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 모형 튜닝 자동화\n",
    "# -----------------------------------------------------------------------------\n",
    "# cartesian grid search \n",
    "# -----------------------------------------------------------------------------\n",
    "hyper_parameters = {'max_depth': [4, 6, 8, 12, 16, 20]}\n",
    "\n",
    "# 조합 모형 돌리기\n",
    "m = H2OGridSearch(H2ORandomForestEstimator,\n",
    "                  hyper_params=hyper_parameters,\n",
    "                  search_criteria={'strategy': \"Cartesian\"},\n",
    "                  grid_id='RF_depth_grid')\n",
    "\n",
    "m.train(x = x,\n",
    "        y = y,\n",
    "        training_frame = train_data,\n",
    "        validation_frame = valid_data,\n",
    "        ntrees = 10000,\n",
    "        stopping_rounds = 5,\n",
    "        stopping_tolerance = 1e-4,\n",
    "        stopping_metric = 'AUC',\n",
    "        score_tree_interval = 5,\n",
    "        seed=1111)\n",
    "\n",
    "# AUC가 높은 순으로 정렬하기\n",
    "sortedGrid = m.get_grid(sort_by='auc', decreasing=True)\n",
    "\n",
    "print('===== sortedGrid =====')\n",
    "print(sortedGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Grid Build progress: |███████████████████████████████████████████████████████| (done) 100%\n",
      "Model Details\n",
      "=============\n",
      "H2ORandomForestEstimator : Distributed Random Forest\n",
      "Model Key: RF_grid_model_5\n",
      "\n",
      "\n",
      "Model Summary: \n",
      "    number_of_trees    number_of_internal_trees    model_size_in_bytes    min_depth    max_depth    mean_depth    min_leaves    max_leaves    mean_leaves\n",
      "--  -----------------  --------------------------  ---------------------  -----------  -----------  ------------  ------------  ------------  -------------\n",
      "    60                 60                          34577                  8            8            8             22            60            41.1667\n",
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.009404560154279543\n",
      "RMSE: 0.09697711149688644\n",
      "LogLoss: 0.041451591285602274\n",
      "Mean Per-Class Error: 0.3121981036436281\n",
      "AUC: 0.9285000099389746\n",
      "AUCPR: 0.45444833270136004\n",
      "Gini: 0.8570000198779493\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.27116272666237573\n",
      "       0     1    Error    Rate\n",
      "-----  ----  ---  -------  -------------\n",
      "0      6120  15   0.0024   (15.0/6135.0)\n",
      "1      51    31   0.622    (51.0/82.0)\n",
      "Total  6171  46   0.0106   (66.0/6217.0)\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "metric                       threshold    value     idx\n",
      "---------------------------  -----------  --------  -----\n",
      "max f1                       0.271163     0.484375  42\n",
      "max f2                       0.0874455    0.512295  124\n",
      "max f0point5                 0.30651      0.590551  39\n",
      "max accuracy                 0.347321     0.989545  31\n",
      "max precision                0.994949     1         0\n",
      "max recall                   0.000273374  1         393\n",
      "max specificity              0.994949     1         0\n",
      "max absolute_mcc             0.30651      0.500704  39\n",
      "max min_per_class_accuracy   0.0126656    0.859169  289\n",
      "max mean_per_class_accuracy  0.0143804    0.864482  281\n",
      "max tns                      0.994949     6135      0\n",
      "max fns                      0.994949     81        0\n",
      "max fps                      1.42986e-05  6135      399\n",
      "max tps                      0.000273374  82        393\n",
      "max tnr                      0.994949     1         0\n",
      "max fnr                      0.994949     0.987805  0\n",
      "max fpr                      1.42986e-05  1         399\n",
      "max tpr                      0.000273374  1         393\n",
      "\n",
      "Gains/Lift Table: Avg response rate:  1.32 %, avg score:  1.29 %\n",
      "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
      "-------  --------------------------  -----------------  --------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
      "1        0.0101335                   0.218662           38.5103   38.5103            0.507937         0.439695     0.507937                    0.439695            0.390244        0.390244                   3751.03   3751.03            0.385191\n",
      "2        0.0201062                   0.117535           15.8971   27.2941            0.209677         0.160874     0.36                        0.3014              0.158537        0.54878                    1489.71   2629.41            0.535741\n",
      "3        0.0300788                   0.077474           7.33714   20.6774            0.0967742        0.091812     0.272727                    0.231911            0.0731707       0.621951                   633.714   1967.74            0.599783\n",
      "4        0.0400515                   0.0621246          7.33714   17.3557            0.0967742        0.0687223    0.228916                    0.191278            0.0731707       0.695122                   633.714   1635.57            0.663826\n",
      "5        0.0500241                   0.0526214          0         13.8957            0                0.0574272    0.18328                     0.164594            0               0.695122                   -100      1289.57            0.65372\n",
      "6        0.100048                    0.0196721          1.70649   7.80111            0.022508         0.0324785    0.102894                    0.0985361           0.0853659       0.780488                   70.6494   680.111            0.689534\n",
      "7        0.150072                    0.0126247          1.70649   5.76957            0.022508         0.0157232    0.0760986                   0.0709318           0.0853659       0.865854                   70.6494   476.957            0.725348\n",
      "8        0.200097                    0.00921872         0.48757   4.44907            0.00643087       0.0107417    0.0586817                   0.0558843           0.0243902       0.890244                   -51.243   344.907            0.699372\n",
      "9        0.299984                    0.00610927         0.366266  3.0896             0.00483092       0.00751528   0.0407507                   0.0397786           0.0365854       0.926829                   -63.3734  208.96             0.635224\n",
      "10       0.400032                    0.00377801         0.365677  2.40834            0.00482315       0.00488835   0.0317652                   0.0310525           0.0365854       0.963415                   -63.4323  140.834            0.570913\n",
      "11       0.50008                     0.00178569         0.121892  1.95091            0.00160772       0.00267665   0.0257317                   0.0253755           0.0121951       0.97561                    -87.8108  95.0906            0.481885\n",
      "12       0.599968                    0.000862529        0         1.6261             0                0.00124465   0.0214477                   0.021358            0               0.97561                    -100      62.6103            0.380663\n",
      "13       0.700016                    0.000451205        0         1.3937             0                0.000641308  0.0183824                   0.0183971           0               0.97561                    -100      39.3696            0.279277\n",
      "14       0.799903                    0.000218296        0.244177  1.25015            0.00322061       0.000315373  0.016489                    0.0161392           0.0243902       1                          -75.5823  25.0151            0.202771\n",
      "15       0.899952                    0.000103201        0         1.11117            0                0.00016042   0.0146559                   0.0143628           0               1                          -100      11.1171            0.101385\n",
      "16       1                           0                  0         1                  0                4.93069e-05  0.0131896                   0.0129308           0               1                          -100      0                  0\n",
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.008744636081831514\n",
      "RMSE: 0.09351275892535475\n",
      "LogLoss: 0.03388723910414494\n",
      "Mean Per-Class Error: 0.1509317902218982\n",
      "AUC: 0.9763293388618035\n",
      "AUCPR: 0.5277886304707804\n",
      "Gini: 0.952658677723607\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.12164029578367874\n",
      "       0     1    Error    Rate\n",
      "-----  ----  ---  -------  -------------\n",
      "0      1281  10   0.0077   (10.0/1291.0)\n",
      "1      5     12   0.2941   (5.0/17.0)\n",
      "Total  1286  22   0.0115   (15.0/1308.0)\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "metric                       threshold    value     idx\n",
      "---------------------------  -----------  --------  -----\n",
      "max f1                       0.12164      0.615385  21\n",
      "max f2                       0.0851061    0.721649  28\n",
      "max f0point5                 0.298802     0.714286  7\n",
      "max accuracy                 0.298802     0.99159   7\n",
      "max precision                0.298802     0.875     7\n",
      "max recall                   0.00783992   1         241\n",
      "max specificity              0.640528     0.999225  0\n",
      "max absolute_mcc             0.0851061    0.624542  28\n",
      "max min_per_class_accuracy   0.0389049    0.941176  72\n",
      "max mean_per_class_accuracy  0.0389049    0.946576  72\n",
      "max tns                      0.640528     1290      0\n",
      "max fns                      0.640528     17        0\n",
      "max fps                      0.000113525  1291      399\n",
      "max tps                      0.00783992   17        241\n",
      "max tnr                      0.640528     0.999225  0\n",
      "max fnr                      0.640528     1         0\n",
      "max fpr                      0.000113525  1         399\n",
      "max tpr                      0.00783992   1         241\n",
      "\n",
      "Gains/Lift Table: Avg response rate:  1.30 %, avg score:  1.24 %\n",
      "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
      "-------  --------------------------  -----------------  --------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
      "1        0.0107034                   0.185698           38.4706   38.4706            0.5              0.37575      0.5                         0.37575             0.411765        0.411765                   3747.06   3747.06            0.406343\n",
      "2        0.0206422                   0.102482           35.5113   37.0458            0.461538         0.130015     0.481481                    0.257433            0.352941        0.764706                   3451.13   3604.58            0.753862\n",
      "3        0.030581                    0.0699             5.91855   26.9294            0.0769231        0.0786477    0.35                        0.199328            0.0588235       0.823529                   491.855   2592.94            0.80339\n",
      "4        0.0405199                   0.0557614          0         20.3241            0                0.063514     0.264151                    0.166015            0               0.823529                   -100      1932.41            0.79332\n",
      "5        0.0504587                   0.0461115          0         16.3209            0                0.0489775    0.212121                    0.142962            0               0.823529                   -100      1532.09            0.783251\n",
      "6        0.100153                    0.0232173          2.36742   9.3974             0.0307692        0.0325565    0.122137                    0.0881808           0.117647        0.941176                   136.742   839.74             0.852098\n",
      "7        0.150612                    0.0156523          0         6.24903            0                0.0189669    0.0812183                   0.0649924           0               0.941176                   -100      524.903            0.800975\n",
      "8        0.200306                    0.0111234          0         4.6987             0                0.0131238    0.0610687                   0.0521242           0               0.941176                   -100      369.87             0.750627\n",
      "9        0.300459                    0.0065419          0.587337  3.32824            0.00763359       0.00846876   0.043257                    0.0375724           0.0588235       1                          -41.2663  232.824            0.708753\n",
      "10       0.399847                    0.00433379         0         2.50096            0                0.00542341   0.0325048                   0.0295813           0               1                          -100      150.096            0.608056\n",
      "11       0.5                         0.00185042         0         2                  0                0.00310073   0.0259939                   0.0242771           0               1                          -100      100                0.506584\n",
      "12       0.600153                    0.000957057        0         1.66624            0                0.00132018   0.0216561                   0.020446            0               1                          -100      66.6242            0.405112\n",
      "13       0.699541                    0.000584542        0         1.42951            0                0.000736378  0.0185792                   0.0176458           0               1                          -100      42.9508            0.304415\n",
      "14       0.805046                    0.000308821        0         1.24217            0                0.000442164  0.0161443                   0.0153912           0               1                          -100      24.2165            0.197521\n",
      "15       0.997706                    0.000113594        0         1.0023             0                0.000163026  0.0130268                   0.0124505           0               1                          -100      0.229885           0.00232378\n",
      "16       1                           0.000100704        0         1                  0                0.000104617  0.0129969                   0.0124222           0               1                          -100      0                  0\n",
      "\n",
      "Scoring History: \n",
      "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
      "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
      "    2023-07-13 19:19:21  4.706 sec   0                  nan              nan                 nan             nan                nan              nan                              nan                nan                   nan               nan                  nan                nan\n",
      "    2023-07-13 19:19:21  4.764 sec   5                  0.108798         0.189445            0.814077        0.276116           33.17            0.0164975                        0.0930895          0.0416443             0.90894           0.512245             43.9664            0.00840979\n",
      "    2023-07-13 19:19:21  4.827 sec   10                 0.103154         0.0871076           0.841287        0.333663           30.0861          0.0165746                        0.0934606          0.0337569             0.97314           0.537117             43.9664            0.0107034\n",
      "    2023-07-13 19:19:21  4.888 sec   15                 0.102438         0.0728214           0.858917        0.340796           33.6965          0.0120851                        0.0903051          0.0310952             0.982572          0.613765             43.9664            0.0107034\n",
      "    2023-07-13 19:19:21  4.946 sec   20                 0.101558         0.0662075           0.866182        0.360711           33.6965          0.0122265                        0.091493           0.0315849             0.982549          0.579649             43.9664            0.0107034\n",
      "    2023-07-13 19:19:21  5.014 sec   25                 0.100048         0.0541587           0.903596        0.382777           38.5103          0.0120637                        0.0918193          0.0320774             0.981136          0.601214             43.9664            0.0107034\n",
      "    2023-07-13 19:19:22  5.080 sec   30                 0.0987587        0.0485356           0.91236         0.411717           39.7137          0.0127071                        0.0920885          0.0324711             0.981455          0.597786             43.9664            0.0114679\n",
      "    2023-07-13 19:19:22  5.158 sec   35                 0.0980949        0.0475621           0.917678        0.427094           38.5103          0.0122245                        0.0930425          0.0331951             0.980316          0.519103             38.4706            0.0137615\n",
      "    2023-07-13 19:19:22  5.225 sec   40                 0.0976935        0.0433094           0.917179        0.432061           40.9172          0.0115811                        0.0933479          0.033573              0.978744          0.513564             38.4706            0.0122324\n",
      "    2023-07-13 19:19:22  5.287 sec   45                 0.0979765        0.0426623           0.923632        0.425056           38.5103          0.0119028                        0.0932835          0.0334453             0.978744          0.517502             38.4706            0.0122324\n",
      "    2023-07-13 19:19:22  5.356 sec   50                 0.0972414        0.042062            0.924969        0.447225           37.3068          0.0112594                        0.0933166          0.0334251             0.978084          0.531986             43.9664            0.0129969\n",
      "    2023-07-13 19:19:22  5.430 sec   55                 0.0972519        0.0416537           0.928343        0.45029            37.3068          0.0106161                        0.0933473          0.0336735             0.977058          0.523854             43.9664            0.0137615\n",
      "    2023-07-13 19:19:22  5.508 sec   60                 0.0969771        0.0414516           0.9285          0.454448           38.5103          0.0106161                        0.0935128          0.0338872             0.976329          0.527789             38.4706            0.0114679\n",
      "\n",
      "Variable Importances: \n",
      "variable                         relative_importance    scaled_importance    percentage\n",
      "-------------------------------  ---------------------  -------------------  ---------------------\n",
      "mean_PS_MIN                      116.21441650390625     1.0                  0.060443293970802475\n",
      "mean_AVG_EMP                     78.67771911621094      0.6770048113055439   0.04092040082938865\n",
      "mean_AVG_SURFACE_SALINITY_TREND  62.83058166503906      0.5406436099339463   0.03267828064867338\n",
      "sum_RN_DAY                       39.42790222167969      0.33926859857661823  0.020506511638828714\n",
      "mean_EV_L                        37.647212982177734     0.3239461515595384   0.01958037246942331\n",
      "mean_PA_AVG                      37.110572814941406     0.31932847861172226  0.019301265105977437\n",
      "mean_STDEV_U_VELOCITY_01         35.56123733520508      0.3059967808211624   0.01849545337729273\n",
      "mean_WS_MIN                      35.541072845458984     0.30582326973404683  0.018484965795647013\n",
      "mean_MIN_SALINITY_01             32.826332092285156     0.28246351080876253  0.017073025019838976\n",
      "mean_MIN_U_VELOCITY_01           32.6107177734375       0.2806081960781636   0.016960883686169027\n",
      "---                              ---                    ---                  ---\n",
      "mean_STDEV_TEMP_02               11.526741981506348     0.09918512976502279  0.005995075955919463\n",
      "mean_MIN_U_VELOCITY_03           11.013551712036133     0.09476923813205172  0.005728164919804586\n",
      "mean_MIN_SALINITY_03             10.566509246826172     0.09092253409430495  0.005495657456832385\n",
      "mean_MAX_V_VELOCITY_01           10.333663940429688     0.0889189504305806   0.0053745542604507965\n",
      "mean_AVG_V_VELOCITY_03           9.957467079162598      0.08568185754155469  0.005178893703348609\n",
      "mean_MIN_U_VELOCITY_04           9.409150123596191      0.0809637083474917   0.004893713224613755\n",
      "mean_MAX_SALINITY_02             9.22422981262207       0.07937250893749505  0.00479753589090916\n",
      "mean_AVG_U_VELOCITY_04           8.947687149047852      0.07699291893572514  0.004653705632902199\n",
      "mean_AVG_U_VELOCITY_03           8.098691940307617      0.06968749819464438  0.004212141939468657\n",
      "mean_MAX_V_VELOCITY_03           6.0172247886657715     0.05177692208662871  0.0031295677225854344\n",
      "[82 rows x 4 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 모형 튜닝 자동화\n",
    "minDepth = 12\n",
    "maxDepth = 16\n",
    "\n",
    "# options for grid search \n",
    "max_runtime_secs = 60*10\n",
    "max_models = 100\n",
    "\n",
    "# random grid search \n",
    "hyper_params = {\n",
    "    'max_depth': list(range(minDepth, maxDepth + 1)),\n",
    "    'sample_rate': [i * 0.01 for i in range(20, 100 + 1)],\n",
    "    'col_sample_rate_per_tree': [i * 0.01 for i in range(20, 100 + 1)],\n",
    "    'col_sample_rate_change_per_level': [i * 0.01 for i in range(90, 110 + 1)],\n",
    "    'min_rows': [1,5,10,20,50,100],\n",
    "    'min_split_improvement': [0,1e-8,1e-6,1e-4],\n",
    "    'histogram_type': ['UniformAdaptive', 'QuantilesGlobal', 'RoundRobin']\n",
    "}\n",
    "\n",
    "search_criteria = {\n",
    "    'strategy': \"RandomDiscrete\",\n",
    "    'max_runtime_secs': max_runtime_secs,\n",
    "    'max_models': max_models\n",
    "}\n",
    "\n",
    "grid = H2OGridSearch(H2ORandomForestEstimator\n",
    "                     , hyper_params=hyper_parameters\n",
    "                     , search_criteria=search_criteria\n",
    "                     , grid_id='RF_grid')\n",
    "grid.train(\n",
    "      x = x\n",
    "    , y = y\n",
    "    , training_frame = train_data\n",
    "    , validation_frame = valid_data\n",
    "    , ntrees = 10000\n",
    "    , stopping_rounds = 5\n",
    "    , stopping_tolerance = 1e-4\n",
    "    , stopping_metric = 'AUC'\n",
    "    , score_tree_interval = 5\n",
    "    , seed = 1111\n",
    ")\n",
    "\n",
    "# AUC가 높은 순으로 정렬하기\n",
    "sortedGrid = grid.get_grid(sort_by='auc', decreasing=True)\n",
    "RF_AB_Tune = h2o.get_model(sortedGrid.model_ids[1])\n",
    "print(RF_AB_Tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n",
      "[[1295    7]\n",
      " [  14    9]]               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1302\n",
      "           1       0.56      0.39      0.46        23\n",
      "\n",
      "    accuracy                           0.98      1325\n",
      "   macro avg       0.78      0.69      0.73      1325\n",
      "weighted avg       0.98      0.98      0.98      1325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================================\n",
    "# forecast\n",
    "#======================================================================================================\n",
    "pred= RF_AB_Tune.predict(test_data)\n",
    "pred=pred.as_data_frame()\n",
    "test_data=test_data.as_data_frame()\n",
    "\n",
    "pred = pd.concat([pred['predict'], pred['p1'], test_data['Y']], axis=1)\n",
    "pred.columns = ['Yhat','p1','Y']\n",
    "\n",
    "#confusion matrix\n",
    "print(confusion_matrix(pred['Yhat'],pred['Y']),\n",
    "      classification_report(pred['Yhat'],pred['Y']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
